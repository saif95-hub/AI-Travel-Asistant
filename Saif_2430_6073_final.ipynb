{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mea-CemjMt15"
      },
      "source": [
        "# **LAB 3: Language Representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTCiwRXMdlVS"
      },
      "source": [
        "**Language Representation** a.k.a. Text Representation is the process of converting unstructured text data into a structured format (machine-readable form). It involves converting words, phrases, or entire documents into numerical or symbolic representations while preserving meaning and context.\n",
        "\n",
        "It comprise preprocessing the text data followed by selecting a suitable representation scheme, such as Bag-of-Words, TF-IDF etc. to capture the key features and characteristics of the same, in a numerical form that can be processed by machine learning algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUPIYDoTM3yo"
      },
      "source": [
        "# **Objectives:**\n",
        "This lab is designed to introduce students to  fundamental techniques for representing text in a machine-readable format. These techniques form the foundation for various NLP applications, enabling machines to understand and process human language efficiently\n",
        "By the end of this lab, students will be familiar with several key Language Representation tasks which include:\n",
        "\n",
        "1. Text Preprocessing\n",
        "    * Remove Punctuation\n",
        "    * Remove URLs\n",
        "    * Lowercasing\n",
        "    * Tokenization\n",
        "    * Remove Stop Words\n",
        "    * Stemming\n",
        "    * Lemmatization\n",
        "2. Character Encoding\n",
        "    * ASCII\n",
        "    * UTF-8\n",
        "3. Text Representation\n",
        "    * Bag-of-Words (BoW)\n",
        "    * Term Frequency - Inverse Document Frequency (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  **Text Preprocessing**\n",
        "Raw text data is often messy and unstructured, so we need Text Preprocessing, as it cleans and organizes text for better analysis and predictions\n",
        "\n",
        "\n",
        "* Remove Punctuation\n",
        "* Remove URLs\n",
        "* Lowercasing\n",
        "* Tokenization\n",
        "* Remove Stop Words\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "\n"
      ],
      "metadata": {
        "id": "ogTyWzhdWAdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove Punctuation**"
      ],
      "metadata": {
        "id": "7Z_70YsO2c3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello, world! How's it going?\"\n",
        "text_no_punct = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(text_no_punct)  # Output: Hello world Hows it going"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXJT-gNIY2Nt",
        "outputId": "75ad8ff4-038f-4a42-d5f6-cdc184e72877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world Hows it going\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove URLs**"
      ],
      "metadata": {
        "id": "uySpPCrt2hUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Check this out: https://example.com for more details.\"\n",
        "text_no_urls = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "print(text_no_urls)  # Output: Check this out:  for more details."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbiZcvsla4N0",
        "outputId": "08c26cbc-ac4b-4845-f703-2a78a78065f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check this out:  for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lowercasing**"
      ],
      "metadata": {
        "id": "n-J0Xp6l2yeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"HELLO World! This is an Example.\"\n",
        "text_lower = text.lower()\n",
        "print(text_lower)  # Output: hello world! this is an example."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOVaPSq1biHL",
        "outputId": "e2c0984c-8214-4472-985e-b03316460d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world! this is an example.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**"
      ],
      "metadata": {
        "id": "8SuBpf0D6i73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19T3pnaJ6pX7",
        "outputId": "c93c23f2-1af9-4d45-f330-4660810d980b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is an example sentence. to demonstrate stop word removal.\"\n",
        "words = word_tokenize(text)\n",
        "print(words) # Output: ['this', 'is', 'an', 'example', 'sentence', 'to', 'demonstrate', 'stop', 'word', 'removal', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InXXv3ia8egR",
        "outputId": "12a22526-c293-4b94-f935-5405383db792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'an', 'example', 'sentence', '.', 'to', 'demonstrate', 'stop', 'word', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Remove Stop Words**"
      ],
      "metadata": {
        "id": "wk0AINHp2rKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the `words` from previous block\n",
        "filtered_text = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "print(filtered_text)  # Output: ['example', 'sentence', 'demonstrate', 'stop', 'word', 'removal', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDR-VcehbfOR",
        "outputId": "98ed8342-a388-4af5-858f-c02b6af27c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', '.', 'demonstrate', 'stop', 'word', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**"
      ],
      "metadata": {
        "id": "IciaoQfU28AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "nyUSq5y42TPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"better\", \"happily\", \"jumping\", \"countries\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)\n",
        "# Output: ['run', 'fli', 'better', 'happili', 'jump', 'countri']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtfw5foQ9xWG",
        "outputId": "2790cc15-521c-4bef-8189-bd2b40c1bc44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'better', 'happili', 'jump', 'countri']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatization**"
      ],
      "metadata": {
        "id": "aIB6tNBK2-xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld0X-6qq2Vj0",
        "outputId": "bd2792ab-cc52-429b-f0e2-11650cdcf98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"flies\", \"better\", \"happily\", \"jumping\", \"countries\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"n\") for word in words]  # 'n' for noun\n",
        "\n",
        "print(lemmatized_words)  # Output: ['run', 'fly', 'better', 'happily', 'jump']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkDPa4cU9X5A",
        "outputId": "1600de05-e49c-450e-808d-43ca2da91b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'fly', 'better', 'happily', 'jumping', 'country']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Valid options for `pos` in `.lemmatize()` are ‚Äún‚Äù for nouns, ‚Äúv‚Äù for verbs, ‚Äúa‚Äù for adjectives, ‚Äúr‚Äù for adverbs and ‚Äús‚Äù for satellite adjectives.\n",
        "\n"
      ],
      "metadata": {
        "id": "12h10fYe_w6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task: Perform the Preprocessing Steps sequentially, on the provided example, by following the hints in comments**"
      ],
      "metadata": {
        "id": "B-Jgwzw8m1I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltpQpSmam0uF",
        "outputId": "8ad21255-903e-4fa8-ef3c-24a2f1933947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Text to be used for preprocessing\n",
        "text = \"Hello, world! NLP is amazing. Let's learn it at https://example.com.\"\n",
        "\n",
        "# 1. Remove Punctuation from Provided Text\n",
        "text_no_punct = re.sub(r'[^\\w\\s]', '', text)\n",
        "print(\"\\nStep 1: Text without Punctuation:\")\n",
        "print(text_no_punct)\n",
        "\n",
        "# 2. Remove URLs from Output of Step 1\n",
        "text_no_urls = re.sub(r'https?://\\S+', '', text_no_punct)\n",
        "print(\"\\nStep 2: Text without URLs:\")\n",
        "print(text_no_urls)\n",
        "\n",
        "# 3. Perform Lowercasing on Output of Step 2\n",
        "text_lower = text_no_urls.lower()\n",
        "print(\"\\nStep 3: Lowercased Text:\")\n",
        "print(text_lower)\n",
        "\n",
        "# 4. Perform Word and Sentence Tokenization, individually on Output of Step 3\n",
        "words = word_tokenize(text_lower)\n",
        "sentences = sent_tokenize(text_lower)\n",
        "print(\"\\nStep 4: Word Tokens:\")\n",
        "print(words)\n",
        "print(\"\\nStep 4: Sentence Tokens:\")\n",
        "print(sentences)\n",
        "\n",
        "# 5. Remove Stop Words from Output of Step 4 (word tokenize output)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "print(\"\\nStep 5: Text without Stop Words:\")\n",
        "print(filtered_words)\n",
        "\n",
        "# 6. Perform Stemming on Output of Step 5\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"\\nStep 6: Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# 7. Perform Lemmatization on Output of Step 5, making sure POS tag is set for Verb\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
        "print(\"\\nStep 7: Lemmatized Words:\")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "id": "AkStK_f8-G4y",
        "outputId": "be48511d-c5c8-49f9-a47c-78f7ffc5e0cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Text without Punctuation:\n",
            "Hello world NLP is amazing Lets learn it at httpsexamplecom\n",
            "\n",
            "Step 2: Text without URLs:\n",
            "Hello world NLP is amazing Lets learn it at httpsexamplecom\n",
            "\n",
            "Step 3: Lowercased Text:\n",
            "hello world nlp is amazing lets learn it at httpsexamplecom\n",
            "\n",
            "Step 4: Word Tokens:\n",
            "['hello', 'world', 'nlp', 'is', 'amazing', 'lets', 'learn', 'it', 'at', 'httpsexamplecom']\n",
            "\n",
            "Step 4: Sentence Tokens:\n",
            "['hello world nlp is amazing lets learn it at httpsexamplecom']\n",
            "\n",
            "Step 5: Text without Stop Words:\n",
            "['hello', 'world', 'nlp', 'amazing', 'lets', 'learn', 'httpsexamplecom']\n",
            "\n",
            "Step 6: Stemmed Words:\n",
            "['hello', 'world', 'nlp', 'amaz', 'let', 'learn', 'httpsexamplecom']\n",
            "\n",
            "Step 7: Lemmatized Words:\n",
            "['hello', 'world', 'nlp', 'amaze', 'let', 'learn', 'httpsexamplecom']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Text to be used for preprocessing\n",
        "text = \"Hello, world! NLP is amazing. Let's learn it at https://example.com.\"\n",
        "\n",
        "# 1. Remove Punctuation from Provided Text\n",
        "text_no_punct = re.sub(r'[^\\w\\s]', '', text)\n",
        "print(\"\\nStep 1: Text without Punctuation:\")\n",
        "print(text_no_punct)\n",
        "\n",
        "# 2. Remove URLs from Output of Step 1\n",
        "text_no_urls = re.sub(r'https?://\\S+', '', text_no_punct)\n",
        "print(\"\\nStep 2: Text without URLs:\")\n",
        "print(text_no_urls)\n",
        "\n",
        "# 3. Perform Lowercasing on Output of Step 2\n",
        "text_lower = text_no_urls.lower()\n",
        "print(\"\\nStep 3: Lowercased Text:\")\n",
        "print(text_lower)\n",
        "\n",
        "# 4. Perform Word and Sentence Tokenization, individually on Output of Step 3\n",
        "words = word_tokenize(text_lower)\n",
        "sentences = sent_tokenize(text_lower)\n",
        "print(\"\\nStep 4: Word Tokens:\")\n",
        "print(words)\n",
        "print(\"\\nStep 4: Sentence Tokens:\")\n",
        "print(sentences)\n",
        "\n",
        "# 5. Remove Stop Words from Output of Step 4 (word tokenize output)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "print(\"\\nStep 5: Text without Stop Words:\")\n",
        "print(filtered_words)\n",
        "\n",
        "# 6. Perform Stemming on Output of Step 5\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"\\nStep 6: Stemmed Words:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# 7. Perform Lemmatization on Output of Step 5, making sure POS tag is set for Verb\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
        "print(\"\\nStep 7: Lemmatized Words:\")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "outputId": "91edbd68-b919-4dec-c456-4d80f5c3540a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f_CDxqsWngn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Text without Punctuation:\n",
            "Hello world NLP is amazing Lets learn it at httpsexamplecom\n",
            "\n",
            "Step 2: Text without URLs:\n",
            "Hello world NLP is amazing Lets learn it at httpsexamplecom\n",
            "\n",
            "Step 3: Lowercased Text:\n",
            "hello world nlp is amazing lets learn it at httpsexamplecom\n",
            "\n",
            "Step 4: Word Tokens:\n",
            "['hello', 'world', 'nlp', 'is', 'amazing', 'lets', 'learn', 'it', 'at', 'httpsexamplecom']\n",
            "\n",
            "Step 4: Sentence Tokens:\n",
            "['hello world nlp is amazing lets learn it at httpsexamplecom']\n",
            "\n",
            "Step 5: Text without Stop Words:\n",
            "['hello', 'world', 'nlp', 'amazing', 'lets', 'learn', 'httpsexamplecom']\n",
            "\n",
            "Step 6: Stemmed Words:\n",
            "['hello', 'world', 'nlp', 'amaz', 'let', 'learn', 'httpsexamplecom']\n",
            "\n",
            "Step 7: Lemmatized Words:\n",
            "['hello', 'world', 'nlp', 'amaze', 'let', 'learn', 'httpsexamplecom']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  **Character Encoding**\n",
        "Raw text data is often messy and unstructured, so we need Text Preprocessing, as it cleans and organizes text for better analysis and predictions\n",
        "\n",
        "\n",
        "* American Standard Code for Information Interchange (ASCII)\n",
        "* Unicode Transformation Format 8 (UTF-8)"
      ],
      "metadata": {
        "id": "HlyctAVVY3fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **American Standard Code for Information Interchange (ASCII)**\n",
        "\n",
        "ASCII is a character encoding standard that uses binary numbers to represent text, and is used in computers, telecommunications, and other devices."
      ],
      "metadata": {
        "id": "_QwuLFRaEoUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform ASCII encoding and decoding using the `.encode()` and `.decode()` function, where the encoding type is set as `\"ascii\"`.\n",
        "\n",
        "Due to its limited multilingual support, it can not encode Non-ASCII characters, so we need to ignore them!"
      ],
      "metadata": {
        "id": "_zgNGKCnHSmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, ÿØŸÜ€åÿß\"\n",
        "\n",
        "# Encode using ASCII (ignoring non-ASCII characters)\n",
        "encoded_text = text.encode(\"ascii\", errors=\"ignore\")\n",
        "print(\"Encoded Text: \", encoded_text)  # Output: b'Caf'\n",
        "\n",
        "# Encode using ASCII (replacing non-ASCII characters)\n",
        "decoded_text = encoded_text.decode(\"ascii\")\n",
        "print(\"Decoded Text: \", decoded_text)  # Output: Hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u79AyCo6xt7u",
        "outputId": "c4cfb886-25dd-42f7-df68-bc9b48f35d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Text:  b'Hello, '\n",
            "Decoded Text:  Hello, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Unicode Transformation Format 8 (UTF-8)**\n",
        "\n",
        "UTF-8 is a character encoding standard which leverages variable-width encoding, meaning that each character is represented by one to four bytes. It is the most common encoding for the World Wide Web."
      ],
      "metadata": {
        "id": "YzEPaHWhEnY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to ASCII, we can perform UTF-8 encoding and decoding using the `.encode()` and `.decode()` function, where the encoding type is set as `\"UTF-8\"`\n",
        "\n",
        "Due to its vast multilingual support, it can encode Non English characters too!\n",
        "\n"
      ],
      "metadata": {
        "id": "aFzcyuKeIOjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Task: Perform UTF-8 Encoding and Decoding on the provided example, using the hints in comments**"
      ],
      "metadata": {
        "id": "nlpjaPQ9JKfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, ÿØŸÜ€åÿß\"\n",
        "\n",
        "# Encode using ASCII (ignoring non-ASCII characters)\n",
        "encoded_text = text.encode(\"ascii\", errors=\"ignore\")\n",
        "print(\"Encoded Text: \", encoded_text)  # Output: b'Caf'\n",
        "\n",
        "# Encode using ASCII (replacing non-ASCII characters)\n",
        "decoded_text = encoded_text.decode(\"ascii\")\n",
        "print(\"Decoded Text: \", decoded_text)  # Output: Hello"
      ],
      "metadata": {
        "id": "tdfclDuUyJ2Z",
        "outputId": "9e7396da-c4bf-439a-ca1a-083bf6981bc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Text:  b'Hello, '\n",
            "Decoded Text:  Hello, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Text Representation**\n",
        "\n",
        "Text representation is the process of converting unstructured text data into a structured format (machine-readable form) that can be used for natural language processing tasks\n",
        "\n",
        "It involves selecting a suitable representation scheme, such as bag-of-words, TF-IDF, word embeddings, or topic models, to capture the key features and characteristics of the text data in a numerical form that can be processed by machine learning algorithms.\n",
        "\n",
        "\n",
        "a) **Bag-of-Words (BoW) Representation:**\n",
        "\n",
        "It represents text as a vector of word frequencies, ignoring grammar and word order, based on a corpus-wide vocabulary.\n",
        "\n",
        "\n",
        "b) **Term Frequency - Inverse Document Frequency (TF-IDF) Representation:**\n",
        "\n",
        "It is a statistical measure that evaluates a word's importance in a document relative to a collection of documents by combining its frequency in the document (TF) and its rarity across the corpus (IDF).\n",
        "\n",
        "Words that appear frequently across many documents (common words) have lower importance."
      ],
      "metadata": {
        "id": "ygOZDTxNf_tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example:**\n",
        "\n",
        "**Input Text 1:** \"I love NLP.\"\n",
        "\n",
        "**Input Text 2:** \"NLP is good.\"\n",
        "\n",
        "a) **Bag-of-Words (BoW) Representation:**\n",
        "\n",
        "Assuming the above 2 sentences where \"NLP\" is common, while other words are occurring once, the vector assign equal weight to \"NLP\" as the other words.\n",
        "\n",
        "b) **Term Frequency - Inverse Document Frequency (TF-IDF) Representation:**\n",
        "\n",
        "Assuming the above 2 sentences where \"NLP\" is common, while other words are occurring one, the vector assign lower weight to \"NLP\", as compared to other words.\n",
        "\n",
        "Now we will implement BoW and TF-IDF in this lab."
      ],
      "metadata": {
        "id": "i5ChAcYmNczt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BoW**"
      ],
      "metadata": {
        "id": "tgXiBAngRF_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer # for BoW"
      ],
      "metadata": {
        "id": "5yjPtE0WLmwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input texts\n",
        "text1 = \"I love NLP.\"\n",
        "text2 = \"NLP is an interesting subject.\"\n",
        "\n",
        "# Bag of Words (BoW)\n",
        "\n",
        "# Initialize the CountVectorizer, which converts text into a matrix of token counts\n",
        "bow_vectorizer = CountVectorizer()\n",
        "# Fit and transform the input texts into a BoW matrix\n",
        "bow_matrix = bow_vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "# Feature names and BoW representation\n",
        "print(\"Bag of Words (BoW):\")\n",
        "print(\"Feature Names:\", bow_vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRtvl7UYJyUA",
        "outputId": "66bbcf17-ad65-44da-b1c6-185c9c2e5ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW):\n",
            "Feature Names: ['an' 'interesting' 'is' 'love' 'nlp' 'subject']\n",
            "BoW Matrix:\n",
            " [[0 0 0 1 1 0]\n",
            " [1 1 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: `vectorizer.fit_transform()` build a unique vocabulary by\n",
        "  * Applying Tokenization\n",
        "  * Removing Duplicates\n",
        "  * Lowercasing\n",
        "  * Stop Word Removal"
      ],
      "metadata": {
        "id": "wWeBbQGBOhAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF-IDF**"
      ],
      "metadata": {
        "id": "5rZRySOfQ2ED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Task: Perform TF-IDF on the above example (used in BoW), using the hints in comments**"
      ],
      "metadata": {
        "id": "DS_hXDJvN5V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer # for TF-IDF"
      ],
      "metadata": {
        "id": "o1zb-5XuOwYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import subprocess\n",
        "import webbrowser\n",
        "import asyncio\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import dotenv_values\n",
        "from groq import Groq\n",
        "from googlesearch import search as gsearch\n",
        "from json import load, dump\n",
        "\n",
        "# Load .env variables\n",
        "env_vars = dotenv_values(\".env\")\n",
        "GroqAPIKey = env_vars.get(\"GroqAPIKey\")\n",
        "Username = env_vars.get(\"Username\", \"Traveler\")\n",
        "Assistantname = env_vars.get(\"Assistantname\", \"TravelMate\")\n",
        "\n",
        "client = Groq(api_key=GroqAPIKey)\n",
        "\n",
        "# Chat history\n",
        "chatlog_path = \"Data/ChatLog.json\"\n",
        "os.makedirs(\"Data\", exist_ok=True)\n",
        "if not os.path.exists(chatlog_path):\n",
        "    with open(chatlog_path, \"w\") as f:\n",
        "        dump([], f)\n",
        "\n",
        "SystemChatBot = [\n",
        "    {\"role\": \"system\", \"content\": f\"\"\"You are {Assistantname}, a professional travel assistant helping users plan trips, check destinations, weather, flights, and itineraries. Respond clearly, with proper grammar, punctuation, and professionalism.\"\"\"}\n",
        "]\n",
        "\n",
        "def RealTimeInfo():\n",
        "    now = datetime.datetime.now()\n",
        "    return f\"Current Time: {now.strftime('%A, %B %d, %Y - %H:%M:%S')}\"\n",
        "\n",
        "def ChatBot(query):\n",
        "    try:\n",
        "        with open(chatlog_path, \"r\") as f:\n",
        "            messages = load(f)\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"llama3-70b-8192\",\n",
        "            messages=SystemChatBot + [{\"role\": \"system\", \"content\": RealTimeInfo()}] + messages,\n",
        "            max_tokens=1200,\n",
        "            temperature=0.7,\n",
        "            stream=True\n",
        "        )\n",
        "\n",
        "        response = \"\"\n",
        "        for chunk in completion:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                response += chunk.choices[0].delta.content\n",
        "\n",
        "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "        with open(chatlog_path, \"w\") as f:\n",
        "            dump(messages, f, indent=4)\n",
        "\n",
        "        return response.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {e}\"\n",
        "\n",
        "def GoogleSearch(query):\n",
        "    results = list(gsearch(query, advanced=True, num_results=3))\n",
        "    return \"\\n\\n\".join([f\"üìå {r.title}\\n{r.description}\" for r in results])\n",
        "\n",
        "def GetWeather(city):\n",
        "    try:\n",
        "        url = f\"https://wttr.in/{city}?format=3\"\n",
        "        res = requests.get(url)\n",
        "        return res.text if res.status_code == 200 else \"Could not fetch weather.\"\n",
        "    except:\n",
        "        return \"Weather request failed.\"\n",
        "\n",
        "def GenerateItinerary(destination):\n",
        "    query = f\"Create a 3-day travel itinerary for {destination} including places to visit, local food, and tips.\"\n",
        "    return ChatBot(query)\n",
        "\n",
        "def SearchHotels(destination):\n",
        "    query = f\"Hotels in {destination}\"\n",
        "    webbrowser.open(f\"https://www.google.com/search?q={query}\")\n",
        "    return f\"Searching hotels in {destination}...\"\n",
        "\n",
        "def SearchFlights(destination):\n",
        "    webbrowser.open(f\"https://www.google.com/flights?q=flights+to+{destination}\")\n",
        "    return f\"Opening Google Flights for {destination}...\"\n",
        "\n",
        "async def ExecuteCommand(commands):\n",
        "    funcs = []\n",
        "\n",
        "    for cmd in commands:\n",
        "        cmd = cmd.lower()\n",
        "        if cmd.startswith(\"weather \"):\n",
        "            city = cmd.removeprefix(\"weather \")\n",
        "            funcs.append(asyncio.to_thread(GetWeather, city))\n",
        "        elif cmd.startswith(\"itinerary \"):\n",
        "            dest = cmd.removeprefix(\"itinerary \")\n",
        "            funcs.append(asyncio.to_thread(GenerateItinerary, dest))\n",
        "        elif cmd.startswith(\"hotels \"):\n",
        "            dest = cmd.removeprefix(\"hotels \")\n",
        "            funcs.append(asyncio.to_thread(SearchHotels, dest))\n",
        "        elif cmd.startswith(\"flights \"):\n",
        "            dest = cmd.removeprefix(\"flights \")\n",
        "            funcs.append(asyncio.to_thread(SearchFlights, dest))\n",
        "        elif cmd.startswith(\"search \"):\n",
        "            topic = cmd.removeprefix(\"search \")\n",
        "            funcs.append(asyncio.to_thread(GoogleSearch, topic))\n",
        "        else:\n",
        "            funcs.append(asyncio.to_thread(ChatBot, cmd))\n",
        "\n",
        "    results = await asyncio.gather(*funcs)\n",
        "    for result in results:\n",
        "        if result:\n",
        "            print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"{Assistantname} Ready! Type 'exit' to quit.\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in (\"exit\", \"quit\"):\n",
        "            break\n",
        "        asyncio.run(ExecuteCommand([user_input]))\n"
      ],
      "metadata": {
        "id": "9qR5T6Dszsct",
        "outputId": "e57ffda8-db7f-449b-f012-c94f91f26217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-107db759093a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdotenv_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgroq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgooglesearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgsearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8PX1U0clGgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}